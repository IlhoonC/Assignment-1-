{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Fri Jan 20 13:40:18 2023\n\"\"\"\nimport os\nimport numpy as np\nimport pandas as pd\nimport scipy.optimize as opt\n\nclass ChoiceModels(object):\n    \n    '''\n    This class defines methods that will be used later in speficying and estimating choice models.\n    '''\n   \n    def load_data(self, path, file):\n        df = pd.read_csv(os.path.join(path, file), sep='\\s+', header=0)\n        df['cons'] = 1.\n        return df\n    \n    def expand_data(self, df, n):\n        '''\n        Parameters\n        ----------\n        df : a pandas data frame\n            \n        n : Integer\n            Number of times to expand the data\n\n        Returns\n        -------\n        An expanded pandas data frame with a panel structure\n\n        '''\n        df['Alt'] = [[str(i) for i in range(n)] for _ in range(len(df))]\n        return df.explode('Alt')\n    \n    def create_choice_attributes(self, df, config):\n        '''\n        This method creates a panel structure of data to estimate the multinomial\n        choice model speficied in the configuration file (config-- a json format file)\n        '''\n        # create dependent variable\n        y_namelist = list(config['Alternatives']['0'].keys())\n        df['choice'] = list(zip(*[df[v] for v in y_namelist]))\n        df = self.expand_data(df, len(config['Alternatives']))\n   \n        df['y'] = 0.\n        for k,v in config['Alternatives'].items():\n            label = tuple(v.values())\n            df.loc[(df[\"Alt\"]==k) & (df['choice']==label), 'y'] = 1\n        \n        # create alternative specific attributes\n        dic = config['Attributes']\n        for var,info in dic.items():\n            df[var] = 0\n            for alt, w in info['rule'].items():\n                df['junk'] = 0\n                df.loc[(df['Alt'] == alt), 'junk'] = 1\n                df[var] = df[var] + w * df[info['variable']] * df['junk'] \n        df = df.drop(\"junk\", axis='columns')\n        \n        # creat interactions\n        df, xz_list = self.create_interactions(df, config['Interactions']) \n        x_list = ['cons'] + list(config['Attributes'].keys()) + xz_list\n        return {'data': df, \"var_names\": x_list}\n    \n    def create_interactions(self, df, interact_list):\n        '''\n        Parameters\n        ----------\n        df : pandas data frame\n            \n        interact_list : a List\n            The list contains pairs of variable names as tuples\n\n        Returns\n        -------\n        df : pandas data frame after adding interactions\n            \n        xz_list : A list of created interactions\n\n        '''\n        xz_list = []\n        for item in interact_list:\n            vname = item[0] + \"_\" + item[1]\n            df[vname] = df[item[0]] * df[item[1]]\n            xz_list.append(vname)\n        return df, xz_list \n        \n        \n    def optimization(self, objfun, para):\n        '''\n        Parameters\n        ----------\n        objfun : a user defined objective function of para\n            \n        para : a 1-D array with the shape (k,), where k is the number of parameters.\n\n        Returns\n        -------\n        dict\n            A dictionary containing estimation results\n\n        '''\n        v = opt.minimize(objfun, x0=para, jac=None, method='BFGS', \n                          options={'maxiter': 1000, 'disp': True})  \n        return {'log_likelihood':-1*v.fun, \"Coefficients\": v.x, \"Var_Cov\": v.hess_inv}\n\n    \nclass BinaryLogit(ChoiceModels):\n    '''\n    This class is to estimate a binary logit nodel by MLE.  \n    '''\n    def __init__(self, path, file, yname, x=None, z=None, interactions=None):\n        df = super().load_data(path, file)\n        if x is None:\n            x = []\n        if z is None:\n            z = []\n        if interactions is None:\n            xz = []\n            self.df = df\n        else:\n            self.df, xz = super().create_interactions(df, interactions)\n            \n        self.X_list = ['cons'] + x + z + xz\n        self.Xmat = self.df[self.X_list].to_numpy()\n        self.y = self.df[yname].to_numpy()\n        \n    def log_likelihood(self, para):\n        '''\n        Parameters\n        ----------\n        para : array\n            a 1-D array with the shape(k,), where k is the number of model parameters.\n\n        Returns\n        -------\n        res : scalar\n            log-likelihood value\n\n        '''\n        xb = np.matmul(self.Xmat, para)\n        xb = np.exp(xb)\n        xb = xb / (1+xb)\n        return (-1/len(xb)) * np.sum(self.y * np.log(xb) + (1-self.y) * np.log(1 - xb))\n   \n    def estimation(self, para):\n        '''\n        Parameters\n        ----------\n        para : array\n            a 1-D array with the shape(k,), where k is the number of model parameters.\n\n        Returns\n        -------\n        A dictionary of estimation results\n        '''\n        return super().optimization(self.log_likelihood, para)\n\n    \nclass MultinomialLogit(ChoiceModels):\n\n    # Specify model here    \n    model_config = {\"Alternatives\":\n                    {\"0\": {\"occupanc\": 1, \"route\": 1},\n                     \"1\": {\"occupanc\": 1, \"route\": 0},\n                     \"2\": {\"occupanc\": 2, \"route\": 1},\n                     \"3\": {\"occupanc\": 2, \"route\": 0},\n                     \"4\": {\"occupanc\": 3, \"route\": 1},\n                     \"5\": {\"occupanc\": 3, \"route\": 0}},\n                    \"Attributes\":{'express_dummy':{'variable':'cons', \n                                                   'rule':{\"0\":1,\"2\":1,\"4\":1}},\n                                  'hov2_dummy':{'variable':'cons', \n                                               'rule':{\"2\":1,\"3\":1}},\n                                  \"hov3_dummy\":{'variable':'cons', \n                                                'rule':{\"4\":1,\"5\":1}},\n                                  \"price\":{\"variable\": 'toll', \n                                           \"rule\": {\"0\":1,\"2\":1/2,\"4\":1/6}},\n                                  \"time\": {\"variable\":\"median\", \n                                           \"rule\":{\"0\":1,\"2\":1,\"4\":1}}},\n                    \"Interactions\":[('price', \"high_income\")]}\n    \n    \n    def __init__(self, path, file):\n        df = super().load_data(path, file)\n        res = super().create_choice_attributes(df, MultinomialLogit.model_config)\n        self.df = res['data']\n        self.X_list = res['var_names']\n        self.y = self.df['y'].to_numpy()\n        self.Xmat = self.df[self.X_list].to_numpy()\n               \n    def mnl_log_likelihood(self, para):\n        '''\n        This method defines the data log-likelihood from a Multinomial Logit.\n        '''\n        df = self.df.copy()\n        xb = np.matmul(self.Xmat, para)\n        xb = np.exp(xb)\n        df['xb'] = xb.tolist()\n        # group sum\n        df['xbsum'] = df.groupby(['id'])[\"xb\"].transform(lambda x: x.sum())\n        df['log_likelihood'] = df['y']*np.log(df['xb'] / df['xbsum'])\n        return (-1/len(df))* np.sum(df['log_likelihood'])\n \n    def estimation(self, para):\n        '''\n        Parameters\n        ----------\n        para : array\n            a 1-D array with the shape(k,), where k is the number of model parameters.\n\n        Returns\n        -------\n        A dictionary of estimation results\n        '''\n        return super().optimization(self.mnl_log_likelihood, para)\n\n\nclass NestedLogit(ChoiceModels):\n\n    # Specify model here    \n    model_config = {\"Alternatives\":\n                    {\"0\": {\"trans\": 1, \"occupanc\": 1, \"route\": 1},\n                     \"1\": {\"trans\": 1, \"occupanc\": 1, \"route\": 0},\n                     \"2\": {\"trans\": 1, \"occupanc\": 2, \"route\": 1},\n                     \"3\": {\"trans\": 1, \"occupanc\": 2, \"route\": 0},\n                     \"4\": {\"trans\": 1, \"occupanc\": 3, \"route\": 1},\n                     \"5\": {\"trans\": 1, \"occupanc\": 3, \"route\": 0},\n                     \"6\": {\"trans\": 0, \"occupanc\": 1, \"route\": 0},\n                     \"7\": {\"trans\": 0, \"occupanc\": 2, \"route\": 0},\n                     \"8\": {\"trans\": 0, \"occupanc\": 3, \"route\": 0}},\n                    \"Attributes\": {'trans_dummy':{'variable': 'cons', \n                                                 'rule':{\"0\":1,\"1\":1,\n                                                         \"2\":1,\"3\":1,\"4\":1,\"5\":1}},\n                                  'express_dummy':{'variable':'cons', \n                                                   'rule':{\"0\":1,\"2\":1,\"4\":1}},\n                                  'hov2_dummy':{'variable':'cons', \n                                               'rule':{\"2\":1,\"3\":1,\"7\":1}},\n                                  \"hov3_dummy\":{'variable':'cons', \n                                                'rule':{\"4\":1,\"5\":1,\"8\":1}},\n                                  \"price\":{\"variable\": 'toll', \n                                           \"rule\": {\"0\":1,\"2\":1/2,\"4\":1/6}},\n                                  \"time\": {\"variable\":\"median\", \n                                           \"rule\":{\"0\":1,\"2\":1,\"4\":1}}},\n                    \"Interactions\":[('price', \"high_income\")]}\n    \n    \n    def __init__(self, path, file):\n        df = super().load_data(path, file)\n        res = super().create_choice_attributes(df, NestedLogit.model_config)\n        self.df = res['data']\n        self.X_list = res['var_names']\n        self.y = self.df['y'].to_numpy()\n        self.Xmat = self.df[self.X_list].to_numpy()\n               \n    def nl_log_likelihood(self, para):\n        '''\n        This method defines the data log-likelihood from a Nested Logit.\n        '''\n        dt = para[0:3]\n        ld = para[3:5]\n        bt = para[5:]\n        \n        df = self.df.copy()\n        xb = np.matmul(self.Xmat, bt)\n        df['xb'] = xb.tolist()\n        \n        df['n1'] = 0\n        df.loc[(df[\"Alt\"]==\"2\") | (df[\"Alt\"]==\"3\"), 'n1'] = 1\n        df.loc[(df[\"Alt\"]==\"4\") | (df['Alt']==\"5\"), 'n1'] = 2\n        df.loc[(df[\"Alt\"]==\"6\") | (df['Alt']==\"7\") | (df['Alt']==\"8\"), 'n1'] = 3\n        \n        df['n2'] = 0\n        df.loc[(df[\"Alt\"]==\"6\") | (df['Alt']==\"7\") | (df['Alt']==\"8\"), 'n2'] = 1\n         \n            \n        #Computing the conditional probability of choosing the road\n        \n        df['t1'] = ((df['xb']/dt[0])*(df['n1']==0)) + ((df['xb']/dt[1])*(df['n1']==1)) + ((df['xb']/dt[2])*(df['n1']==2)) + (df['xb']*(df['n1']==3))\n        df['t1'] = np.exp(df['t1'])\n        df['xbsum1'] = df.groupby(['id','n1'])[\"t1\"].transform(lambda x: x.sum())\n        df['sb_nest_prob'] = ((df['t1']/df['xbsum1'])*(df['n1'] != 3)) + (df['n1'] == 3)\n        \n        #Computing the conditional probability of choosing the occupancy\n        \n        df['sb_nest_IV'] = (df['xbsum1']*(df['n1']!=3)) + (df['t1']*(df['n1']==3)) \n        df['sb_nest_IV'] = np.log(df['sb_nest_IV'])\n        df['t2'] = (df['sb_nest_IV']*(dt[0]/ld[0])*(df['n1']==0)) + (df['sb_nest_IV']*(dt[1]/ld[0])*(df['n1']==1)) + (df['sb_nest_IV']*(dt[2]/ld[0])*(df['n1']==2)) + (df['sb_nest_IV']*(1/ld[1])*(df['n1']==3))\n        df['t2'] = np.exp(df['t2'])\n        df['xbsum2'] = df.groupby(['id','n2'])[\"t2\"].transform(lambda x: x.sum())\n        df['xbsum2'] = ((df['xbsum2']/2)*(df['n2']==0)) + (df['xbsum2']*(df['n2']==1))\n        df['nest_prob'] = df['t2']/df['xbsum2']\n        \n        #Computing the marginal probability of choosing the transponder\n        \n        df['nest_IV'] = df.groupby(['id','n2'])[\"xbsum2\"].transform(lambda x: x.sum())\n        df['nest_IV'] = ((df['nest_IV']/6)*(df['n2']==0)) + ((df['nest_IV']/3)*(df['n2']==1))\n        df['nest_IV'] = np.log(df['nest_IV'])\n        df['t3'] = (ld[0]*df['nest_IV']*(df['n2']==0)) + (ld[1]*df['nest_IV']*(df['n2']==1))\n        df['t3'] = np.exp(df['t3'])\n        \n        df['junk2'] = (df['t3']*(df['Alt']==\"0\")) + (df['t3']*(df['Alt']==\"6\"))\n        df['xbsum3'] = df.groupby(['id'])[\"junk2\"].transform(lambda x: x.sum())\n        \n        df['prob_tp'] = df['t3']/df['xbsum3'] \n        \n        #Computing the probability of choosing an alternative i\n        \n        df['prob'] = df['sb_nest_prob'] * df['nest_prob'] * df['prob_tp']\n        \n        #Computing the log_likelihood\n        \n        df['log_likelihood'] = df['y']*np.log(df['prob'])\n        return (-1/len(df))* np.sum(df['log_likelihood'])\n \n    def estimation(self, para):\n        '''\n        Parameters\n        ----------\n        para : array\n            a 1-D array with the shape(k,), where k is the number of model parameters.\n\n        Returns\n        -------\n        A dictionary of estimation results\n        '''\n        return super().optimization(self.nl_log_likelihood, para)    \n\n    \nclass MixedLogit(ChoiceModels):\n\n    # Specify model here    \n    model_config = {\"Alternatives\":\n                    {\"0\": {\"occupanc\": 1, \"route\": 1},\n                     \"1\": {\"occupanc\": 1, \"route\": 0},\n                     \"2\": {\"occupanc\": 2, \"route\": 1},\n                     \"3\": {\"occupanc\": 2, \"route\": 0},\n                     \"4\": {\"occupanc\": 3, \"route\": 1},\n                     \"5\": {\"occupanc\": 3, \"route\": 0}},\n                    \"Attributes\":{'express_dummy':{'variable':'cons', \n                                                   'rule':{\"0\":1,\"2\":1,\"4\":1}},\n                                  'hov2_dummy':{'variable':'cons', \n                                               'rule':{\"2\":1,\"3\":1}},\n                                  \"hov3_dummy\":{'variable':'cons', \n                                                'rule':{\"4\":1,\"5\":1}},\n                                  \"price\":{\"variable\": 'toll', \n                                           \"rule\": {\"0\":1,\"2\":1/2,\"4\":1/6}},\n                                  \"time\": {\"variable\":\"median\", \n                                           \"rule\":{\"0\":1,\"2\":1,\"4\":1}}},\n                                   \"Interactions\":[]}\n                    \n    \n    \n    def __init__(self, path, file):\n        df = super().load_data(path, file)\n        res = super().create_choice_attributes(df, MixedLogit.model_config)\n        self.df = res['data']\n        self.X_list = res['var_names']\n        self.y = self.df['y'].to_numpy()\n        self.Xmat = self.df[self.X_list].to_numpy()\n               \n    def mixed_log_likelihood(self, para):\n        '''\n        This method defines the data log-likelihood from a Mixed Logit.\n        '''\n        df = self.df.copy()\n        df['prob_update'] = 0\n        g = len(self.X_list)-2\n        \n        for i in range(100):\n            cof_price = -np.random.lognormal(para[g], para[g+1])\n            cof_time = -np.random.lognormal(para[g+2], para[g+3])\n            beta = para[0:g] \n            beta = np.append(beta, cof_price)\n            beta = np.append(beta, cof_time)\n            xb = np.matmul(self.Xmat, beta)\n            xb = np.exp(xb)\n            df['xb'] = xb.tolist()\n            # group sum\n            df['xbsum'] = df.groupby(['id'])[\"xb\"].transform(lambda x: x.sum())\n            df['prob']  = df['xb']/df['xbsum']\n            df['prob_update'] = df['prob'] + df['prob_update']\n            #print(df['prob_update'].head(9))\n        \n        df['prob_update'] = df['prob_update']/100\n        df['log_likelihood'] = df['y']*np.log(df['prob_update'])    \n        return (-1/len(df))* np.sum(df['log_likelihood'])\n \n    def estimation(self, para):\n        '''\n        Parameters\n        ----------\n        para : array\n            a 1-D array with the shape(k,), where k is the number of model parameters.\n\n        Returns\n        -------\n        A dictionary of estimation results\n        '''\n        return super().optimization(self.mixed_log_likelihood, para)    \n\n    \nif __name__ == '__main__':\n    p = \"/kaggle/input/assignment-1\"\n    f = \"assignment 1.txt\"\n    ## estimating binary models\n    x = ['toll', 'median']\n    z = ['female', 'age3050']\n    interactions = [('toll', 'high_income')]\n    route = BinaryLogit(p, f, \"trans\", x=x, z=z, interactions=interactions)\n    bini = np.zeros(len(route.X_list))\n    res_binary = route.estimation(bini)\n    \n    ## estimating a MNL model\n    mnl = MultinomialLogit(p, f)\n    bini = np.ones(len(mnl.X_list))*(-0.05)\n    res_mnl = mnl.estimation(bini)\n    \n    ## estimating a NL model\n    nl = NestedLogit(p,f)\n    bini_nl = np.ones(len(nl.X_list)+5)*0.5\n    res_nl = nl.estimation(bini_nl)\n    \n    ## estimating a mixed logit model\n    mxl = MixedLogit(p,f) \n    bini_mix = [0,0,0,0,0,1,0,1]\n    res_mix = mxl.estimation(bini_mix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}